{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Generative Adversarial Networks (GANs)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Yann LeCun, a prominent French computer scientist considers GANs as the coolest thing since sliced bread. The popularity of generative GANs and the quality of the results they produce today surely echo Lecun's claim. Adversarial training used in GANs has revolutionized the way in which neural networks were being used for solving complex problems. GANs do not work by calculating probability density estimation like VAEs. Instead, it is based on **Game Theory** approach with an objective to find **Nash equilibrium** between the two networks, a **Generator** and  a **Discriminator** (Yes, we are talking about Nobel prize winner [John Nash](https://en.wikipedia.org/wiki/John_Forbes_Nash_Jr.), and his game theory as highlighted in popular movie \"A Beautiful Mind\"). GANs allow sampling from a simple distribution like Gaussian and learn to transform this noise to data distribution using neural networks.\n",
    "\n",
    "So what are Generative Adversarial Networks ? What makes them so “interesting” ?, how do these differ from VAEs in their approach and what are the limitations in training GANs.  We will try to answer these questions in this lesson. \n",
    "\n",
    "## Objectives \n",
    "\n",
    "You will be able to:\n",
    "-  Describe Game theory and NAsh equilibrium, Minimax game in context of GANs\n",
    "-  LEarn the dofference between the training functions used by generators and discriminators in GANs.\n",
    "-  Understand how Gans differ from VAEs and other such architectures in terms of their learning approach\n",
    "-  Highlight possible challenges with training GANs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why GANs ?\n",
    "\n",
    "GANs have recently been getting a lot of attention since their introduction by [Ian Goodfellow](https://arxiv.org/abs/1406.2661) in 2014, the results are impressive and also promising. From generating images to increasing resolutions to image translation, GANs have not failed to surprise the deep learning community. Neural Networks had made a great progress before inception of GANs. CNNs, RNNs, RBMs and VAEs - all appeared on scene before GANs were introduced. These classical architectures were capable of recognizing images and voice at levels comparable to humans, and understand natural language with high accuracy.\n",
    "\n",
    "Even with such advanced architectures, the idea of automating cognitive human tasks with machines looks a bit too far fetched. Human cognition involves more than just identifying image, interpreting sounds or understanding what people around us are saying. Let us see a few examples where we need human cognition and creativity:\n",
    "\n",
    "- Train an AI writer to produce some descriptions and automatically generate image data based on this description for audience in a very intuitive manner, which involves learning from images and captions seen previously.\n",
    "\n",
    "<img src=\"birds.png\" width=700>\n",
    "\n",
    "- Create an artificial artist (painter) which can paint like any famous artist by learning their painting style from from original work. \n",
    "\n",
    "\n",
    "<img src=\"mona.jpg\" width=500>\n",
    "\n",
    "- Identify what a person will look like in 20 years based on data collected on effects on aging. \n",
    "\n",
    "<img src=\"age.png\" width=500>\n",
    "\n",
    "These are highly challenging tasks, but GANs have started making some of these tasks possible.\n",
    "\n",
    "*\"One day we'll be talking about good old \"hand-crafted\" films and instead the norm will be watching AI-generated (infinite) content on demand\"* Andrej Karpathy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GANs Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into the main topic of this lesson, which is about GANs, we need to illustrate some definitions and models that make up a complete GAN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Theory\n",
    "\n",
    "Game theory is the science of strategy. It attempts to determine mathematically and logically the actions that “players” should take to secure the best outcomes for themselves in a wide array of “games.” The games it studies range from chess to tennis and other strategic games. But the games all share the common feature of interdependence. That is, the outcome for each participant depends on the choices (strategies) of all. In so-called zero-sum games the interests of the players conflict totally, so that one person’s gain always is another’s loss. More typical are games with the potential for either mutual gain (positive sum) or mutual harm (negative sum), as well as some conflict.\n",
    "\n",
    "In the early years the emphasis was on games of pure conflict (zero-sum games). Other games were considered in a cooperative form. That is, the participants were supposed to choose and implement their actions jointly. Recent research has focused on games that are neither zero sum nor purely cooperative. In these games the players choose their actions separately, but their links to others involve elements of both competition and cooperation.\n",
    "\n",
    "> **A general principle for a player in a sequential-move game is to look ahead and reason back.** \n",
    "\n",
    "Each player should figure out how the other players will respond to his current move, how he will respond in turn, and so on. The player anticipates where his initial decisions will ultimately lead and uses this information to calculate his current best choice. \n",
    "\n",
    "Poker is a great example because the other players choices influences your strategy. For example, should you play tight while your opponent is playing loose? Or, should you bluff or not? Or, raise/fold?\n",
    "\n",
    "![](game.png)\n",
    "\n",
    "Althoough game theory is a huge topic, we will restrict our discussion to the key game types that would enable us understand how GANs work under such a setup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash Equilibrium \n",
    "\n",
    "> __Nash Equilibrium conceptual term that is used in Game Theory to describe a game situation in which the game players are satisfied by the decision they make after revealing the other players strategies. Post this equilibrium, each player has no intention to change the strategy after knowing the other strategies as they didn’t effect on the strategy the player used to win the game.__\n",
    "\n",
    "For example, assume we have a game in which each player has 2 choices to choose between them. The 2 choices are valid and have the same effect on the game points or rewards. The first player strategy maybe to choose the first choice, while the other player’s choice is the second one. After revealing that, each player is satisfied by the strategy they took because the other’s choice hasn’t effect badly them. \n",
    "\n",
    "So in short, **Nash Equilibrium determines the optimal solution in a non-cooperative game in which each player lacks any incentive to change his/her initial strategy.**. Nash equilibrium is one the fundamental concepts in game theory. It conceptualizes the behavior and interactions between game participants to determine the best outcomes. It also allows predicting the decisions of the players if they are making decisions at the same time and the decision of one player takes into account the decisions of other players.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Imagine two rival companies A and B. Both companies want to determine whether to launch a new advertising campaign for their products. If both companies start advertising, each company will attract 100 new customers. If only one company decides to advertise, it will attract 200 new customers, while the other company will not attract any new customers. If both companies decide not to advertise, none of the companies will engage new customers. This information helps us create a **Payoff table** as shown below:\n",
    "\n",
    "![](pay.jpg)\n",
    "\n",
    "Company A should advertise its products because the strategy provides a better payoff than the option of not advertising. The same situation exists for Company B. Thus, the scenario when both companies advertise their products is a Nash equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax\n",
    "\n",
    "Minimax is an algorithm used to implement ideas from the game theory and Statistics. The algorithm is used in games with 2 players, and each player tries to win the game by minimizing the worst case that is provided by the other player move, in other words, the player Minimize the Maximum move of the other player.\n",
    "\n",
    "You can imagine the game of Chess, in which each player tries to win by making the best available move while the other player tries to minimize this move which is considered to be the best move by his side. Minimax is commonly used when making an AI-bot agent in Chess, Tic-tak-toc and Connect-4 games, you can generalize in the decision-making rule-based games.\n",
    "\n",
    "#### Example\n",
    "\n",
    "In 1943, the Allied forces received reports that a Japanese convoy would be heading by sea to reinforce their troops. The convoy could take on of two routes, the Northern or the Southern route. The Allies had to decide where to disperse their reconnaissance aircraft in the north or the south, in order to spot the convoy as early as possible. The following payoff matrix shows the possible decisions made by the Japanese and the Allies, with the outcomes expressed in the number of days of bombing the Allies could achieve with each possibility:\n",
    "\n",
    "![](mm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this matrix, if the Japanese chose to take the southern route while the Allies decided to focus their recon planes in the north, the convoy would be bombed for 2 days. The best outcome for the Allies would be if they placed their airplanes in the south and the Japanese took the southern route. The best outcome for the Japanese would be to take the northern route, provided the Allies were looking for them in the south.\n",
    "\n",
    "To minimize the worst possible outcome, the Allies would have to choose the north as the focus of their reconnaisance efforts. This ensures them 2 days of bombing, whereas they risk only 1 day of bombing if they focus on the south. Therefore, by minimax, the best strategy for them would be to focus on the north.\n",
    "\n",
    "The Japanese can use the same strategy. The worst possible outcome for them is the 3 days of bombing which might occur if they took the southern route. Therefore, the Japanese would take the northern route.\n",
    "\n",
    "It is, in fact, what had occurred: both the Allies and the Japanese chose the north, and the Japanese convoy was bombed for 2 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to GANs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs takes up a game-theoretic approach as shown above, unlike a conventional neural network. The network learns to generate from a training distribution through a 2-player game. The two entities are **Generator** and **Discriminator**. These two adversaries are in constant battle throughout the training process. \n",
    "\n",
    "![](gan1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator is used to generate real-looking images and the discriminator’s job is to identify which one is a fake. The two adversaries are in constant battle as generator constantly tries to fool the discriminator, while the discriminator tries not to be fooled. \n",
    "\n",
    "To generate the best images you will need a very good generator and a discriminator. This is because:\n",
    "- If your generator is not good enough, it will never be able to fool the discriminator and the model will never converge. \n",
    "- If the discriminator is bad, then images which make no sense will also be classified as real and hence your model never trains and in turn you never produces the desired output. \n",
    "\n",
    "The input, random noise shown above can be a Gaussian distribution and values can be sampled from this distribution and fed into the generator network and an image is generated. This generated image is compared with a real image by the discriminator and it tries to identify if the given image is fake or real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax Objective Function and Training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the objective function we try to minimize with minimax. \n",
    "<img src=\"func.png\" width=600>\n",
    "\n",
    "\n",
    "Since a game-theoretic approach is taken, our objective function is represented as a minimax function. The discriminator tries to maximize the objective function, therefore we can perform gradient ascent (hill climbing) using the objective function shown below. \n",
    "\n",
    "<img src=\"func3.png\" width=500>\n",
    "\n",
    "The generator tries to minimize the objective function, therefore we can perform gradient descent on the objective function. By alternating between gradient ascent and descent, the network can be trained.\n",
    "\n",
    "<img src=\"func2.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied, it is observed that optimizing the generator objective function does not work so well, this is because when the sample is generated is likely to be classified as fake, the model would like to learn from the gradients but the gradients turn out to be relatively flat. This makes it difficult for the model to learn. Therefore, the generator objective function is changed as below.\n",
    "\n",
    "\n",
    "<img src=\"func5.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of minimizing the likelihood of discriminator being correct, we maximize the likelihood of discriminator being wrong. Therefore, we perform gradient ascent on generator according to this objective function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although GANs are powerful, still there are many challenges with training of GANs. \n",
    "\n",
    "- __Mode Collapse__: Generator learns one data that fools the discriminator and produces several copies of exactly the same data.\n",
    "- __Bad Initialization__: The network try to take successive steps to minimize a non-convex objective and end up in an oscillating process rather than decreasing the underlying true objective.\n",
    "- __Problem with Counting__: Sometimes it fails to differentiate number of particular objects that should occur at a location e.g. The number of eyes in the head. Below are few examples of this. \n",
    "\n",
    "<img src=\"p1.jpeg\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem with Perspective**: GANs sometime are not capable of differentiating between different views (e.g. Front and back view).\n",
    "\n",
    "<img src=\"p2.jpeg\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem with Global Structure**: GANs don’t understand a holistic structure.\n",
    "\n",
    "<img src=\"p3.jpeg\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources \n",
    "\n",
    "[Overview of Generative Adversarial Networks](https://arxiv.org/pdf/1710.07035.pdf)\n",
    "\n",
    "[What is a GAN?](https://medium.com/@jonathan_hui/gan-whats-generative-adversarial-networks-and-its-application-f39ed278ef09)\n",
    "\n",
    "[Intro to game theory and Nash equilibrium](http://economics.fundamentalfinance.com/game-theory/introduction-game-theory.php)\n",
    "\n",
    "[Why is it so hard to train GANs](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "GANs are generative models that use supervised learning to approximate an objective function. GAN framework puts two adversaries against each other in a game which are controlled by set of parameters. Typically this functions are implemented as neural networks. The goal of the discriminator is to output the probability that its input is real or fake, while the generator tries to model samples which matches the true data distribution.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
